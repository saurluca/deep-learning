{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsblatt 2 - Programmieraufgaben (15 Punkte)\n",
    "BA-INF 153: Einführung in Deep Learning für Visual Computing\n",
    "\n",
    "**Deadline - 01.05.2023 via eCampus**\n",
    "\n",
    "**Tutoren:**\n",
    " * Alina Pollehn - s6aapoll@uni-bonn.de\n",
    " * Johannes van de Locht - s6jovand@uni-bonn.de\n",
    " \n",
    "**Übungsgruppenleitung:**\n",
    " * Jan Müller - muellerj@cs.uni-bonn.de\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports und Visualisierung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Visualisierung verwenden wir hier [Matplotlib](https://matplotlib.org/). Da wir bisher nur PyTorch und Jupyter in der Conda Kursumgebung installiert haben, muss die Matplotlib noch installiert werden. Dies sollte automatisch beim ersten Ausführen der nächsten Zelle passieren. Es kann notwendig sein Jupyter oder die IDE ihrer Wahl neu zu starten damit die Installation richtig erkannt wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Callable\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "    # Matplotlib ist noch nicht installiert in der aktuellen Umgebung und \n",
    "    # wird mit Hilfe von conda installiert. Dies kann einen Moment dauern.\n",
    "    import sys\n",
    "    %conda install --yes --prefix {sys.prefix} matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# from visualization import (\n",
    "#     create_contourline_figure,\n",
    "#     scatter_path_in_figure,\n",
    "#     create_losscurve_figure,\n",
    "#     create_trapezoid_figure,\n",
    "#     scatter_points_and_labels,\n",
    "#     show_figure\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir uns um die Details der Visualisierung so wenig Gedanken wie nötig machen müssen haben wir die folgenden Funktion gegeben:\n",
    "\n",
    " * ```create_contourline_figure(fn : Callable, log_scale : bool)``` erzeugt eine Konturlinienfigur für eine gegebene Funktion fn. Die Funktion fn muss einen Tensor der Form $(n,2)$ akzeptieren und einen Tensor der Form $(n,)$ mit Skalarwerten zurückgeben. Die Option log_scale bestimmt, ob die Z-Achse logarithmisch skaliert werden soll oder nicht. Die Funktion gibt ein matplotlib.pyplot.Figure-Objekt zurück, das die Konturlinienfigur enthält.\n",
    "\n",
    " * ```scatter_path_in_figure(fig : plt.Figure, path : torch.Tensor, name : str)``` nimmt eine plt.Figure-Instanz, einen Pfad als torch.Tensor und einen Namen als String als Parameter entgegen und gibt eine plt.Figure-Instanz zurück. Die Funktion zeichnet den Pfad als Scatterplot in die übergebene Figur ein und beschriftet ihn mit dem Namen. Die Funktion ist nützlich, um mehrere Pfade in einer einzigen Figur zu vergleichen.\n",
    "\n",
    " * ```create_losscurve_figure(loss_values : list[float])``` erstellt eine Grafik, die den Verlustwert (loss value) in Abhängigkeit von der Anzahl der Trainingsiterationen zeigt. Die Funktion nimmt eine Liste von Gleitkommazahlen als Eingabe, die die Verlustwerte für jede Iteration enthalten. Die Funktion gibt ein plt.Figure-Objekt zurück, das die Grafik darstellt.\n",
    "\n",
    " * ```create_trapezoid_figure(q1: tuple[int], q2: tuple[int], q3: tuple[int], q4: tuple[int])``` erstellt ein Plot eines Trapezes, das durch vier Punkte definiert ist. Sie nimmt die Koordinaten dieser vier Punkte als Eingabe und gibt die entsprechende matplotlib Figure-Instanz zurück.\n",
    "\n",
    " * ```scatter_points_and_labels(fig: plt.Figure, points: torch.Tensor, labels: torch.Tensor, name: str)``` fügt ein Streudiagramm mit Punkten, die je nach zugehörigem Label unterschiedlich gefärbt sind, zu einer matplotlib Figure hinzu.\n",
    "\n",
    " * ```show_figure(fig : plt.Figure)``` zeigt eine matplotlib-Figur an. Die Figur wird als Parameter übergeben und muss vom Typ plt.Figure sein."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum der Rosenbrock Funktion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Rosenbrock-Funktion ist eine differenzierbare, nicht konvexe Funktion, die häufig zum Vergleich von unterschiedlichen Optimierungsalgorithmen verwendet wird. Für Parameter $a,b$ ist die Funktion definiert als\n",
    "\n",
    "\\begin{equation}\n",
    "f(x,y) = (a-x)^2+b(y-x^2)^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Rosenbrock Funktion(1 Punkt):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervollständigen Sie die Funkion ```rosenbrock(...)``` in der untenstehenden Zelle so, dass sie den Funktionswert $f_{a,b}(x,y)$ zurückgibt. Gehen Sie davon aus, dass der Tensor ```xy``` die Form $[2,]$ oder $[n,2]$ haben kann und verändern Sie nicht die Definition der Funktionsargumente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(xy : torch.Tensor, a : float = 1., b : float = 100.) -> torch.Tensor:\n",
    "    result = (a-xy[0])**2 + b(xy[1]-xy[0]**2)**2\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie nun die nächste Codezelle aus um Ihre Implementierung der Rosenbrock Funktion zu test. Die Funktion der Konturlinien der Rosenbrock Funktion anzeigen lassen. Diese sollten wie folgt aussehen:\n",
    "\n",
    "![Kontourlinien der Rosenbrock Funktion mit a = 1 und b = 100 (lograithmisch)](./figures/rosenbrock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_contourline_figure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fig = \u001b[43mcreate_contourline_figure\u001b[49m(rosenbrock, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m show_figure(fig)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_contourline_figure' is not defined"
     ]
    }
   ],
   "source": [
    "fig = create_contourline_figure(rosenbrock, True)\n",
    "show_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Gradient Abstieg mit numerischen Gradienten (3 Punkte):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vervollständigen Sie die untenstehenden Funktionen:\n",
    "\n",
    "* Die Funktion ```numerical_grad2d``` soll den Differenzenquotienten für eine gegebene Funktion ```fn``` an einem Punkt ```xy``` berechnen. Dabei ist ```eps``` die Schrittweite für den Differenzenquotienten.\n",
    "\n",
    "* Die Funktion ```minimize_numerical``` soll das Minimum der Funktion ```fn``` von einem Startpunkt ```xy``` mit Hilfe des Gradientenabstiegs-Verfahren approximieren. Das Gradientenabstiegs-Verfahren soll die Schrittgröße/Lernrate ```lr``` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad2d(fn: Callable, xy: torch.Tensor, eps: float = 1e-4) -> torch.Tensor:\n",
    "    xy_grad = torch.empty_like(xy)\n",
    "    # Begin Ihrer Lösung\n",
    "    # Ende Ihrer Lösung\n",
    "    return xy_grad\n",
    "\n",
    "def minimize_numerical(fn: Callable, xy : torch.Tensor, lr: float = 1e-4, steps : int = 1000) -> torch.Tensor:\n",
    "    path = torch.empty(size=[steps,2])\n",
    "    for idx in range(steps):\n",
    "        path[idx] = xy.detach().clone()\n",
    "        xy_grad = numerical_grad2d(fn, xy)\n",
    "        # Begin Ihrer Lösung\n",
    "        # Ende Ihrer Lösung\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Ihre Implementierung zu überprüfen, führen Sie bitte die untenstehende Zelle aus. Die Visualisierung der Optimierungsschritte sollte ähnlich wie im folgenden Graphen aussehen:\n",
    "\n",
    "![](./figures/numerical_path.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = torch.full([2], -3.0)\n",
    "\n",
    "path = minimize_numerical(rosenbrock, xy)\n",
    "\n",
    "fig = create_contourline_figure(rosenbrock, True)\n",
    "fig = scatter_path_in_figure(fig, path, \"xy\")\n",
    "show_figure(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erläuterung - Automatische Differenzierung in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine zentrale Funktion von PyTorch ist die automatische Berechnung von Ableitungen. Die Berechnung einer Ableitung in PyTorch kann in drei Schritte aufgeteilt werden: \n",
    "\n",
    "1. Wir erzeugen Tensoren, die wir als Eingabe für unsere Berechnungen verwenden und für die wir später den Gradient berechnet werden wollen. Bevor wir eine Tensor zum ersten Mal in einer Berechnung verwenden müssen wir PyTorch mitteilen welche, dass wir für diesen Tensor einen Gradienten berechnen wollen. Dafür hat jeder Tensor hat das boolean Attribute [torch.Tensor.requires_grad](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html) welches angibt ob für diesen Tensor der Gradient ausgewertet werden soll. Dieses Attribut kann entweder beim Erstellen des Tensors oder nachträglich wie folgt festgelegt werden:\n",
    "\n",
    "```python\n",
    "        th_tensor = torch.ones([2], requires_grad = True) # Beim Erzeugen eines Tensors\n",
    "        th_tensor.requires_grad_(True) # Nach dem der Tensor schon erzeugt wurde.\n",
    "```\n",
    "\n",
    "2. Wir können beliebige mathematische Operationen auf dem Tensor ausführen, um eine Ausgabe zu erhalten. PyTorch wird automatisch einen Berechnungsgraphen erstellen, der die Abhängigkeiten zwischen den Tensoren und den Operationen speichert (dazu mehr auf dem nächsten Übungsblatt). \n",
    "\n",
    "    * Die Berechnungen dürfen ausschließlich PyTorch Tensoren und Funktionen verwenden. Operatoren ```+,-,*,/,**``` etc., die für den Tensor überladen sind, sind in den PyTorch Funktionen eingeschlossen.  Ebenfalls sind Operationen zwischen skalaren Konstanten und Tensoren wie zum Beispiel ```y = 2.0*x``` sind zulässig.\n",
    "\n",
    "    * Eine Einschränkung dieser Methode ist, dass wir nur den Gradienten eines skalaren Werts (eine Zahl) berechnen können. Also muss das Ergebnis ein Tensor sein, der nur einen Skalar enthält. d.h. die Form des resultierenden Tensors muss $()$ oder $(1)$ sein.\n",
    "\n",
    "3. Um den Gradienten der Ausgabe in Bezug auf den Eingangstensor zu berechnen, müssen wir die Methode [torch.Tensor.backward()](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) aufrufen, die den Gradienten der Berechnung in dem Attribut [torch.Tensor.grad](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html) speichert.\n",
    "\n",
    "Eine weitere Einschränkung ist, dass wir die Methode `backward()` nur einmal pro Berechnungsgraph aufrufen können, es sei denn, wir setzen den Parameter `retain_graph=True`. Andernfalls wird der Graph nach dem Aufruf gelöscht, um Speicherplatz zu sparen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - PyTorch Autograd(1 Punkt):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwenden Sie die automatische Differenzierung von PyTorch um die Ableitung der Rosenbrock Funktion an der Stelle $(-3,-3)$ zu berechnen und auszugeben. \n",
    "\n",
    "**Hinweis**: Stellen Sie sicher, dass Ihre Rosenbrockfunktion nur PyTorch Funktionen verwendet und passen Sie Ihre Implementierung ggf. an. Wenn Ihre Implementierung korrekt ist, sollten Sie die Ableitungen $df/dx = 14408$ uns $df/dy=-2400$ im grad-Attribute stehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = torch.full([2], -3.)\n",
    "# Beginn Ihrer Lösung\n",
    "# Ende Ihrer Lösung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erläuterung - Gradientabstieg in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir nun in der Lage sind, mit Hilfe von PyTorch die Ableitung der Rosenbrock-Funktion an einem Punkt $(x,y)$ zu berechnen, sind wir unserem Ziel, ein Minimum der Rosenbrock-Funktion zu finden, einen Schritt näher gekommen. Alles, was wir jetzt noch benötigen, ist ein Algorithmus um ein Update der Position durch Gradientabstieg durchzuführen. \n",
    "\n",
    "Jeder Optimierungsalgorithmus in PyTorch erbt von der Klasse [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer) und implementiert die essentiellen Funktionen:\n",
    "\n",
    " * Die Methode [torch.optim.Optimizer.step()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) aktualisiert die Parameter des Modells, basierend auf den berechneten Gradienten.\n",
    "\n",
    " * Die Methode [torch.optim.Optimizer.zero_grad()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) setzt alle Gradienten der Parameter des Optimizers auf Null. Dies ist nützlich, um die Gradientenakkumulation zu vermeiden, die bei mehreren Aufrufen von backward() ohne einen Optimierungsschritt auftreten kann. Die zero_grad() Funktion sollte vor jedem Optimierungsschritt aufgerufen werden, um sicherzustellen, dass die Gradienten korrekt berechnet werden.\n",
    "\n",
    "\n",
    "In der Vorlesung haben wurden vier unterschiedliche Gradient Descent Optimierer vorgestellt, die alle in PyTorch implementiert sind:\n",
    "\n",
    " 1. [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) - einfacher (stochastischer) Gradientenabstieg\n",
    " 2. [torch.optim.Adagrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html) - AdaGrad Algorithmus\n",
    " 3. [torch.optim.RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) - RMSProp Algorithmus\n",
    " 4. [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) - Adam Algorithmus\n",
    "\n",
    "Der Konstruktor dieser Optimierer erwartet ein Iterable Objekt, das die Parameter enthält, die wir optimieren wollen - also zum Beispiel eine Liste von Tensoren. Zusätzlich akzeptiert jeder Optimierer eine Lernrate als Argument. Hier ist ein Beispiel:\n",
    "```python\n",
    "    optimizer = torch.optim.SGD([tensor], lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Minimum der Rosenbrock-Funktion mit PyTorch (4 Punkte):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vervollständigen Sie die Funktion ```minimize``` in der nächsten Codezelle. Die Funktion soll den Funktionswert der Rosenbrock-Funktion an der Stelle ```xy``` auswerten und die Ableitung des Funktionswerts bzgl. des Tensors `xy` bestimmen. Verwenden Sie anschließend den ```optimizer``` um einen Gradientabstieg-Schritt durchzuführen und die Gradienten der Tensoren auf Null zurückzusetzen.\n",
    "\n",
    "* Verwenden Sie den Optimierer SGD, um das Minimum der Rosenbrockfunktion durch Gradientabstieg zu finden. Die Optimierung soll vom Punkt $(-3,-3)$ starten. Verwenden Sie dabei die Funktion ```minimize```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_pytorch(xy : torch.Tensor, optimizer : torch.optim.Optimizer, steps : int = 1000) -> torch.Tensor:\n",
    "    path = torch.empty(size=[steps,2])\n",
    "    for idx in range(steps):\n",
    "        path[idx] = xy.detach().clone()\n",
    "        # Beginn Ihrer Lösung\n",
    "        # Ende Ihrer Lösung\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = torch.full([2], -3.0)\n",
    "\n",
    "# Begin Ihrer Lösung\n",
    "# Ende Ihrer Lösung\n",
    "\n",
    "fig = create_contourline_figure(rosenbrock, True)\n",
    "fig = scatter_path_in_figure(fig, path, \"xy\")\n",
    "show_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Ihre Implementierung korrekt funktioniert sollte die Visualisierung der Optimierungsschritte ähnlich wie die folgenden Figure aussehen:\n",
    "\n",
    "![](./figures/pytorch_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trapez Klassifizierung mit einem Multilayer Prezeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erläuterung über nn.Module, nn.Linear und nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Module ist die Basisklasse für alle neuronalen Netzwerke in PyTorch. Sie definiert die gemeinsame Schnittstelle und Funktionalität für alle Module, wie z.B. Parameterverwaltung, Vorwärts- und Rückwärtsberechnung, Gerätezuordnung und Serialisierung. Ein Modul kann andere Module als Attribute enthalten, um eine hierarchische Struktur zu erzeugen. Ein Modul kann auch benutzerdefinierte Methoden definieren, um seine Logik zu implementieren. torch.nn.Module ist die zentrale Klasse für Deep Learning in PyTorch, da sie die Grundlage für die Erstellung und das Training von komplexen Modellen bietet. In der folgenden Aufgabe werden wir die folgenden Funktionen der Klasse nn.Module verwenden:\n",
    "\n",
    "* `torch.nn.Module.forward()` - Diese Funktion definiert die Berechnung, die das Modul auf den Eingabedaten ausführt. Sie muss von jeder Unterklasse überschrieben werden.\n",
    "* `torch.nn.Module.parameters()` - Diese Funktion gibt einen Iterator über alle Parameter des Moduls zurück, die für das Lernen optimiert werden können.\n",
    "* `torch.nn.Module.apply()` - Diese Funktion wendet eine Funktion rekursiv auf jedes Untermodul (wie von .children() zurückgegeben) sowie auf sich selbst an. Ein typischer Anwendungsfall ist die Initialisierung der Parameter eines Modells (siehe auch torch.nn.init).\n",
    "\n",
    "In PyTorch besteht ein MLP im wesentlichen aus einer Aneinanderreihung einer Klasse, die die affine Abbildung implementiert und einer Klasse, die eine nicht-lineare Aktivierungsfunktion darstellt. Die erste wesentliche Klasse ist \n",
    "\n",
    "```python\n",
    "torch.nn.Linear(in_features, out_features)\n",
    "```\n",
    "\n",
    "und implementiert die affine Abbildung $Wx + b$ wobei $W$ die Gewichte, $b$ der Biaswert und $x$ die Eingabe ist. Ihre Argumente `in_features`, `out_features` bestimmen die Dimension des erwarteten Eingabevektors und des Ausgabevektors.\n",
    "\n",
    "Die zweite Klasse sind die Aktivierungsfunktionen z.B.\n",
    "\n",
    "```python\n",
    "    torch.nn.Identity()\n",
    "    torch.nn.ReLU()\n",
    "    torch.nn.Tanh()\n",
    "    torch.nn.Sigmoid()\n",
    "```\n",
    "\n",
    "Wichtig ist, dass man sowohl von der Klasse `Linear` als auch der Aktiverungsfunktionen, erst eine Instanz erstellen muss bevor man Sie auf Daten anwendet. Hier ein Beispiel für eine affine Abbildung: \n",
    "\n",
    "```python\n",
    "    x = torch.rand(10)\n",
    "    linear = torch.nn.Linear(10, 1)\n",
    "    y = linear(x) \n",
    "```\n",
    "Es ist zwar aus technischer Sicht möglich, einzeln Instanzen dieser Klassen als Variablen zu erstellen und diese nacheinander auf die Ausgabe der vorherigen Schicht anzuwenden allerdings ist unpraktisch und ineffizient. Stattdessen werden die Schichten von Neuronalen Netzwerken in einer Klasse zusammengefasst, die von `torch.nn.Module` erbt. Im Konstruktor dieser Klasse werden die Instanzen der oben genannten Klassen erzeugt, und in der `forward(self,x)` Funktion werden diese auf die Daten $x$ angewendet. Außerdem müssen die Schichten nicht als einzeln Klassenvariable gespeichert werden sondern können in eine `torch.nn.ModuleList` eingefügt werden. Einer ModuleList kann man, wie bei einer normalen Liste in Python, mit der Funktion `append` weiter Elemente hinzufügen. Beispiel:\n",
    "\n",
    "```python\n",
    "    layers = torch.nn.ModuleList()\n",
    "    layers.append(torch.nn.Linear(a,b))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Implementierung eines MLPs (2 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweitern Sie die Klasse `TrapezoidClassifier` so dass sie die Architektur der untenstehenden MLPs statt eines linearen Klassifizierers implementiert. Achten Sie darauf, dass bei einem binären Klassifikationsproblem, der Output der MLPs im Intervall $[0,1]$ liegen muss. Wählen Sie daher eine geeignete Aktivierungsfunktion für die Ausgabeschicht des MLPs.\n",
    "\n",
    "![](./figures/mlp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrapezoidClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Beginn Iher Lösung\n",
    "        self.layers.append(nn.Linear(2,1)) # Ersetzen Sie diese Zeile\n",
    "        # Ende Ihrer Lösung\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Implementierung der Trainings- und Evaluierungsfunktion (3 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vervollständigen Sie die Funktion `train_binary`, so dass das Neuronale Netzwerk `model` mit Hilfe des Optimizers `optim` für `num_steps` Iterationen auf den Trainings-Daten ```(train_points, train_labels)``` trainiert wird den `th.nn.BCELoss()` zu minimieren. Zusätzliche soll die Funktion eine Liste der Fehlerwerte, die während des Trainings berechnet wurden, zurückgeben.\n",
    "    \n",
    "* Vervollständigen Sie die Funktion `evaluate` so dass Sie die Genauigkeit des Neuronalen Netzwerks `model` auf den Test-Daten ```(test_points, test_labels)``` . Berechnen Sie die Genauigkeit in dem Sie den Netzwerkoutput runden $z_i$ und mit der korrekten Klasse $y_i$ vergleichen:\n",
    "\n",
    "$$\n",
    "    \\frac{100}{m} \\sum_{i = 1}^m \\mathbb{I}_i \\text{ wobei } \\mathbb{I}_i := \n",
    "    \\begin{cases}\n",
    "    1 & z_i = y_i \\\\ \n",
    "    0 & \\text{sonst}\n",
    "    \\end{cases}.\n",
    "$$\n",
    "\n",
    "Die Funktion soll ebenfalls die gerundenten Netzwerkoutputs zurückgeben.\n",
    "\n",
    "**Hinweis:** Eine Epoche bezeichnet eine vollständige Iteration über die Element des Datensatzes bzw. über die Minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary(\n",
    "    num_steps    : int,\n",
    "    model        : torch.nn.Module,\n",
    "    optimizer    : torch.optim.Optimizer,\n",
    "    train_points : torch.Tensor,\n",
    "    train_labels : torch.Tensor\n",
    ") -> list[float]:\n",
    "    loss_curve = []\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for _ in range(num_steps):\n",
    "        # Beginn Ihrer Lösung\n",
    "        # Ende Ihrer Lösung\n",
    "        loss_curve.append(loss.item())\n",
    "    return loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model        : torch.nn.Module,\n",
    "    test_points : torch.Tensor,\n",
    "    test_labels : torch.Tensor\n",
    ") -> float:\n",
    "    accuracy = 0.\n",
    "    model.eval()\n",
    "    # Beginn Ihrer Lösung\n",
    "    # Ende Ihrer Lösung\n",
    "    model.train()\n",
    "    return accuracy, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe - Evaluierung  des MLPs (1 Punkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Ihre Implementierung des MLP sowie der Trainings- und Evaluierungsfunktion zu testen, führen Sie bitte die folgende Codezelle aus. Das MLP sollte eine Genauigkeit von über 90% auf den Testdaten erreichen.\n",
    "\n",
    "Führen Sie das Experiment fünfmal durch und beschreiben Sie kurz, was Sie dabei beobachten. Insbesondere sollen Sie auf die Kurve der Zielfunktion eingehen und erläutern, warum es Unterschiede zwischen den einzelnen Experimenten gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainings and test data points\n",
    "train_points, train_labels = torch.load(\"./data/trapezoid_train.pt\")\n",
    "test_points, test_labels = torch.load(\"./data/trapezoid_test.pt\")\n",
    "\n",
    "# Create an instance of the model and optimizer\n",
    "model = TrapezoidClassifier()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model and visulaize the resulting loss function\n",
    "loss_curve = train_binary(10000, model, optimizer, train_points, train_labels)\n",
    "fig = create_losscurve_figure(loss_curve)\n",
    "show_figure(fig)\n",
    "\n",
    "# Compute the resulting accuarcy on the test model\n",
    "accuracy, predictions = evaluate(model, test_points, test_labels)\n",
    "print(f\"Accuarcy of the trained model is {accuracy}\")\n",
    "\n",
    "# Visualize test points and the models prediction\n",
    "q1, q2, q3, q4 = (1, 1), (4, 2), (5, 5), (0, 4)\n",
    "fig = create_trapezoid_figure(q1, q2, q3, q4)\n",
    "fig = scatter_points_and_labels(fig, test_points, predictions, \"test\")\n",
    "show_figure(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
